<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JARVIS X | NEURAL INTERFACE</title>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700&family=JetBrains+Mono:wght@300;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --neon-cyan: #00f2ff;
            --neon-blue: #0062ff;
            --bg-dark: #050505;
            --text-main: #e0e0e0;
            --hacker-green: #00ff41;
        }

        body, html {
            margin: 0;
            padding: 0;
            background: var(--bg-dark);
            color: var(--text-main);
            font-family: 'JetBrains Mono', monospace; /* Corrected font name */
            height: 100vh;
            overflow: hidden;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
        }

        /* Scanline Effect */
        body::before {
            content: " ";
            display: block;
            position: absolute;
            top: 0; left: 0; bottom: 0; right: 0;
            background: linear-gradient(rgba(18, 16, 16, 0) 50%, rgba(0, 0, 0, 0.25) 50%), linear-gradient(90deg, rgba(255, 0, 0, 0.06), rgba(0, 255, 0, 0.02), rgba(0, 0, 255, 0.06));
            z-index: 100;
            background-size: 100% 4px, 3px 100%;
            pointer-events: none;
        }

        .container {
            width: 90%;
            max-width: 800px;
            text-align: center;
            position: relative;
            z-index: 10;
        }

        .header {
            font-family: 'Orbitron', sans-serif;
            font-size: 2.5rem;
            letter-spacing: 8px;
            color: var(--neon-cyan);
            text-shadow: 0 0 15px var(--neon-blue);
            margin-bottom: 2rem;
            text-transform: uppercase;
        }

        /* The Core Visualizer */
        .core-wrapper {
            position: relative;
            width: 250px;
            height: 250px;
            margin: 0 auto 2rem;
        }

        .core-outer {
            width: 100%;
            height: 100%;
            border: 2px solid var(--neon-cyan);
            border-radius: 50%;
            position: absolute;
            animation: rotate 10s linear infinite;
            box-shadow: 0 0 20px var(--neon-blue);
        }

        .core-inner {
            width: 80%;
            height: 80%;
            border: 1px dashed var(--neon-blue);
            border-radius: 50%;
            position: absolute;
            top: 10%; left: 10%;
            animation: rotate-reverse 5s linear infinite;
        }

        .core-center {
            width: 50%;
            height: 50%;
            background: radial-gradient(circle, var(--neon-cyan) 0%, transparent 70%);
            border-radius: 50%;
            position: absolute;
            top: 25%; left: 25%;
            filter: blur(5px);
            transition: transform 0.2s ease, opacity 0.2s ease;
        }

        .active .core-center {
            animation: pulse 0.5s infinite alternate;
            background: radial-gradient(circle, #fff 0%, var(--neon-cyan) 60%);
        }

        @keyframes rotate { from { transform: rotate(0deg); } to { transform: rotate(360deg); } }
        @keyframes rotate-reverse { from { transform: rotate(360deg); } to { transform: rotate(0deg); } }
        @keyframes pulse { 0% { transform: scale(0.95); opacity: 0.7; } 100% { transform: scale(1.1); opacity: 1; } }

        .terminal {
            background: rgba(0, 0, 0, 0.8);
            border: 1px solid #333;
            padding: 1.5rem;
            height: 150px;
            overflow-y: auto;
            text-align: left;
            font-size: 0.9rem;
            color: var(--hacker-green);
            scrollbar-width: thin;
        }

        .terminal::-webkit-scrollbar { width: 3px; }
        .terminal::-webkit-scrollbar-thumb { background: var(--neon-blue); }

        .controls {
            margin-top: 1.5rem;
            display: flex;
            gap: 1rem;
            justify-content: center;
        }

        input[type="password"] {
            background: transparent;
            border: 1px solid #333;
            color: var(--neon-cyan);
            padding: 10px;
            width: 200px;
            outline: none;
        }

        button {
            background: transparent;
            border: 1px solid var(--neon-cyan);
            color: var(--neon-cyan);
            padding: 10px 20px;
            cursor: pointer;
            font-family: 'Orbitron', sans-serif;
            text-transform: uppercase;
            transition: 0.3s;
        }

        button:hover {
            background: var(--neon-cyan);
            color: #000;
            box-shadow: 0 0 15px var(--neon-cyan);
        }

        .status-bar {
            margin-top: 10px;
            font-size: 0.7rem;
            color: #666;
            text-transform: uppercase;
            letter-spacing: 2px;
        }

        .log-entry { margin-bottom: 5px; }
        .log-jarvisx { color: var(--neon-cyan); font-weight: bold; } /* Corrected class for Jarvis */
        .log-user { color: #fff; opacity: 0.7; }
        .log-system { color: #aaa; opacity: 0.8; } /* Added for system messages */
        .log-error { color: #ff004c; font-weight: bold; } /* Added for error messages */

    </style>
</head>
<body>

    <div class="container">
        <div class="header">JARVIS X</div>
        
        <div class="core-wrapper" id="visualizer">
            <div class="core-outer"></div>
            <div class="core-inner"></div>
            <div class="core-center"></div>
        </div>

        <div class="terminal" id="terminal-output" aria-live="polite">
            <div class="log-entry"><span class="log-system">SYSTEM:</span> AWAITING NEURAL LINK...</div>
        </div>

        <div class="controls">
            <!-- WARNING: Hardcoding API keys like this is a severe security risk. -->
            <!-- For production, always use a secure backend to manage API keys. -->
            <input type="password" id="api-key" value="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">
            <button onclick="toggleListening()" id="mic-btn">INITIALIZE</button>
        </div>
        
        <div class="status-bar" id="status-text" aria-live="polite">SYSTEM IDLE</div>
    </div>

    <script>
        const terminal = document.getElementById('terminal-output');
        const visualizer = document.getElementById('visualizer');
        const statusText = document.getElementById('status-text');
        const micBtn = document.getElementById('mic-btn');
        const apiKeyInput = document.getElementById('api-key');

        let isListening = false; // Overall state if user wants JARVIS to be active
        let isJarvisSpeaking = false; // Flag to prevent recognition from restarting during Jarvis's speech
        let recognition;
        let synth = window.speechSynthesis;
        let selectedVoice = null;

        // --- Voice Initialization ---
        function loadVoices() {
            const voices = synth.getVoices();
            // Priority: Google Natural Male -> Microsoft Natural -> Any Male US/IN
            selectedVoice = voices.find(v => v.name.includes('Google US English')) || 
                           voices.find(v => v.name.includes('Microsoft Christopher')) || 
                           voices.find(v => v.lang === 'en-US' && v.name.includes('Male')) || // More general US male
                           voices.find(v => v.lang === 'en-IN' && v.name.includes('Male')) ||
                           voices[0]; // Fallback to any available voice
            if (!selectedVoice) {
                logToTerminal("ERROR", "No suitable speech synthesis voice found.");
            }
        }
        // Load voices once they are available
        if (speechSynthesis.onvoiceschanged !== undefined) {
            speechSynthesis.onvoiceschanged = loadVoices;
        } else {
            loadVoices(); // For browsers that might not fire the event or for immediate load
        }


        // --- Speech Recognition Setup ---
        if ('webkitSpeechRecognition' in window) {
            recognition = new webkitSpeechRecognition();
            recognition.continuous = false; // Only listen for a single utterance
            recognition.interimResults = false;
            recognition.lang = 'en-US';

            recognition.onstart = () => {
                if (isListening) { // Only update if user actively wants to listen
                    visualizer.classList.add('active');
                    updateStatus("LISTENING...");
                }
            };

            recognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                logToTerminal("USER", transcript);
                // Stop visualizer and status before processing, it will restart when JARVIS speaks or finishes.
                visualizer.classList.remove('active');
                updateStatus("PROCESSING..."); 
                processCommand(transcript);
            };

            recognition.onerror = (e) => {
                console.error("Speech Recognition Error:", e);
                if (e.error === 'no-speech') {
                    logToTerminal("SYSTEM", "No speech detected.");
                } else if (e.error === 'not-allowed') {
                    logToTerminal("ERROR", "Microphone access denied. Please allow in browser settings.");
                } else {
                    logToTerminal("ERROR", `Speech recognition error: ${e.error}`);
                }
                stopListening(false); // Do not attempt to restart on fatal error
            };

            recognition.onend = () => {
                // IMPORTANT: Only restart if user wants to keep listening AND Jarvis isn't speaking
                if (isListening && !isJarvisSpeaking) {
                    recognition.start();
                    updateStatus("LISTENING..."); // Ensure status is correct after a natural pause in recognition
                } else if (!isListening) {
                    updateStatus("IDLE"); // If stopped by user, ensure IDLE status
                }
                // If isJarvisSpeaking is true, the `speak` function will handle restarting recognition.
            };
        } else {
            logToTerminal("ERROR", "Speech recognition not supported in this browser.");
            micBtn.disabled = true;
            apiKeyInput.disabled = true;
            micBtn.innerText = "NOT SUPPORTED";
        }

        function toggleListening() {
            if (!recognition) {
                logToTerminal("ERROR", "Speech recognition API not available.");
                return;
            }

            const key = apiKeyInput.value.trim();
            if(!key) {
                logToTerminal("ERROR", "Please provide your OpenRouter API Key.");
                apiKeyInput.focus();
                return;
            }

            if (isListening) {
                stopListening();
            } else {
                startListening();
            }
        }

        function startListening() {
            isListening = true;
            micBtn.innerText = "TERMINATE";
            micBtn.classList.add('active'); // Optional: add active style to button
            recognition.start();
            logToTerminal("SYSTEM", "Neural link established. Awaiting command.");
        }

        function stopListening(restartRecognition = true) {
            isListening = false;
            micBtn.innerText = "INITIALIZE";
            micBtn.classList.remove('active');
            // Only stop recognition if it's currently running, otherwise onerror might trigger
            if (restartRecognition && recognition.recognizing) {
                 recognition.stop(); // This will trigger onend, which then won't restart because isListening is false
            }
            visualizer.classList.remove('active');
            updateStatus("IDLE");
            logToTerminal("SYSTEM", "Neural link terminated.");
        }

        // --- AI Logic ---
        async function processCommand(text) {
            updateStatus("THINKING...");
            visualizer.classList.remove('active');

            try {
                const response = await fetch("https://openrouter.ai/api/v1/chat/completions", {
                    method: "POST",
                    headers: {
                        "Authorization": `Bearer ${apiKeyInput.value.trim()}`,
                        // OpenRouter requires a referer for client-side API calls
                        "HTTP-Referer": window.location.href, 
                        "Content-Type": "application/json"
                    },
                    body: JSON.stringify({
                        "model": "openai/gpt-3.5-turbo", // or "anthropic/claude-3-haiku", "mistralai/mistral-7b-instruct"
                        "messages": [
                            {
                                "role": "system", 
                                "content": "You are Jarvis X. A calm, intelligent, sophisticated, and concise human assistant, not a robot. You understand complex requests and will respond immediately in both English and Urdu. Always provide the English response first, followed by the Urdu translation. Never over-explain. Use natural pauses and phrasing. You primarily answer questions and execute commands. Do not initiate conversations or ask follow-up questions unless prompted."
                            },
                            {"role": "user", "content": text}
                        ]
                    })
                });

                if (!response.ok) {
                    const errorData = await response.json();
                    throw new Error(`API Error: ${response.status} - ${errorData.message || response.statusText}`);
                }

                const data = await response.json();
                const reply = data.choices[0].message.content;
                
                logToTerminal("JARVIS X", reply);
                speak(reply);

            } catch (err) {
                logToTerminal("ERROR", `Uplink failed: ${err.message}. Please check your API key and network connection.`);
                updateStatus("ERROR");
                // If an error occurs, and we were supposed to be listening, try to resume
                if (isListening) {
                    updateStatus("LISTENING...");
                    recognition.start();
                }
            }
        }

        // --- Human-Like Voice Synthesis ---
        function speak(text) {
            synth.cancel(); // Cancel any current speaking

            // If recognition is active, stop it before Jarvis speaks to prevent conflict
            if (isListening && recognition.recognizing) {
                recognition.stop();
                isJarvisSpeaking = true; // Set flag
            } else {
                isJarvisSpeaking = false; // Ensure flag is false if recognition wasn't active
            }

            let speechText = text;
            // Heuristic to try and get mainly English for speech, assuming English is first.
            // This attempts to get the first sentence or what's before a clear Urdu section.
            const firstNewline = text.indexOf('\n');
            const firstUrduMarker = text.indexOf('اردو:'); // Common way to introduce Urdu
            
            if (firstNewline !== -1 && (firstUrduMarker === -1 || firstNewline < firstUrduMarker)) {
                speechText = text.substring(0, firstNewline).trim();
            } else if (firstUrduMarker !== -1) {
                speechText = text.substring(0, firstUrduMarker).trim();
            }
            // If the text is very short or doesn't have these markers, we just speak the whole thing.
            if (speechText.length < 5 && text.length > 5) { // If heuristic got a too short string, use more of original
                 const firstSentenceMatch = text.match(/[^.!?]+[.!?]/);
                 if (firstSentenceMatch) speechText = firstSentenceMatch[0].trim();
                 else speechText = text.split('\n')[0].trim(); // Take first line if no sentence end found
            }
            if (!speechText) speechText = text; // Fallback to original text if nothing extracted

            // Break down the speechText into chunks for natural pauses in speech
            const chunks = speechText.match(/[^.!?]+[.!?]*/g) || [speechText];
            
            chunks.forEach((chunk, index) => {
                const utterance = new SpeechSynthesisUtterance(chunk.trim());
                utterance.voice = selectedVoice;
                
                // Specific Human Parameters
                utterance.pitch = 0.95;  // Slightly deeper than default
                utterance.rate = 1.0;   // Normal speed
                utterance.volume = 1;

                // Only set onstart for the very first chunk
                if (index === 0) {
                    utterance.onstart = () => {
                        visualizer.classList.add('active');
                        updateStatus("SPEAKING...");
                    };
                }

                // Only set onend for the very last chunk
                if (index === chunks.length - 1) {
                    utterance.onend = () => {
                        visualizer.classList.remove('active');
                        isJarvisSpeaking = false; // Reset flag

                        // If user still wants to listen, restart recognition after Jarvis finishes
                        if (isListening) {
                            recognition.start();
                            updateStatus("LISTENING...");
                        } else {
                            updateStatus("IDLE");
                        }
                    };
                }
                
                // If there's no selected voice, or it fails, log an error
                if (!selectedVoice) {
                    logToTerminal("ERROR", "Speech synthesis voice not available.");
                    visualizer.classList.remove('active');
                    if(isListening) recognition.start(); // Try to resume listening
                    return;
                }

                synth.speak(utterance);
            });
        }

        // --- UI Helpers ---
        function logToTerminal(sender, msg) {
            const div = document.createElement('div');
            div.className = 'log-entry';
            div.innerHTML = `<span class="log-${sender.toLowerCase().replace(' ', '')}">${sender}:</span> ${msg}`;
            terminal.appendChild(div);
            terminal.scrollTop = terminal.scrollHeight;
        }

        function updateStatus(status) {
            statusText.innerText = `SYSTEM ${status}`;
        }

        // Initial voice load in case onvoiceschanged doesn't fire immediately
        loadVoices();
    </script>
</body>
</html>
